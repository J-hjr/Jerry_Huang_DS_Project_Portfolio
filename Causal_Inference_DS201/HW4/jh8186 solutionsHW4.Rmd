---
title: "jh8186-solutionRMDHW4"
author: "Jerry Huang"
date: "2024-07-23"
output: pdf_document
---

```{r}
library(tidyverse)
```


# Problem 1 - Political Efficacy in China and Mexico
## 1
```{r Question 1}
vignettes <- read.csv("vignettes.csv")

has_na <- anyNA(vignettes) # False, we have no Nah value

China <- vignettes %>%
  filter(china == 1) %>%
  mutate(
    total = self + alison + jane + moses
  )

Mexico <- vignettes %>%
  filter(china == 0) %>%
  mutate(
    total = self + alison + jane + moses
  )

China_mean <- mean(China$self)
Mexico_mean <- mean(Mexico$self)
China_mean
Mexico_mean

# Colours
colours = c("royalblue", "tomato", "orchid", "cyan","springgreen3", "purple")

barplot(prop.table(table(China$self)),
names = c("None", "A little", "Some", "A lot", "Unlimited"),
xlab = "Self-Reported Political Efficacy",
ylab = "Proportion of Respondents", main = "Self-Assessment Responses for Chinese Citizens", col = colours)

barplot(prop.table(table(Mexico$self)),
names = c("None", "A little", "Some", "A lot", "Unlimited"),
xlab = "Self-Reported Political Efficacy",
ylab = "Proportion of Respondents", main = "Self-Assessment Responses for Mexican Citizens", col = colours)


```
The mean response of self-assessment for China and Mexico are 2.621908 and 1.825301 respectively, which shows that China appears to have a higher political efficacy.  
However, this is somewhat different with the historical fact of these two countries. On one hand, the Mexicans were able to vote out the ruling Institutional Revolutionary Party (PRI) who had governed the country for more than 80 years during the 2000 election. On the other hand, Chinese citizens have not been able to vote in a free and fair election. Therefore, this contrast might show that the political efficacy does not align with the political realities all the time and there are other factors influencing how citizens in these two countries perceive their political efficacy.

## 2
```{r}
median_age_China <- median(China$age)
median_age_Mexico <- median(Mexico$age)

hist(China$age, freq = TRUE, main = "Chinese",
     xlim = c(0, 100), ylim = c(0, 60),
     xlab = "Age distribution")
abline(v = median_age_China, col = "cyan", lwd = 3)
text(x = median_age_China * 1.2, y = 60, paste("Median =", round(median_age_China, 2)), col = "blue")

hist(Mexico$age, freq = TRUE, main = "Mexican",
     xlim = c(0, 100), ylim = c(0, 100),
     xlab = "Age distribution")
abline(v = median_age_Mexico, col = "cyan", lwd = 3)
text(x = median_age_Mexico * 1.2, y = 80, paste("Median =", round(median_age_Mexico, 2)), col = "blue")

# Quantile-quantile plot
qqplot(China$age, Mexico$age, xlab = "Chinese", 
       ylab = "Mexican", xlim = c(0, 100), ylim = c(0, 100), 
       main = "Age distribution between China and Mexico")
abline(0, 1) # 45 degree line
```
According to the age distribution of two countries, we can see that Mexico appears to have more young subjects with a median of 35 in the survey compared to the age distribution of Chinese respondents which appears more spread out with a higher number of respondents aged between 30 and 50.  
The Q-Q plot compares the age distribution of Chinese respondents (x-axis) to Mexican respondents (y-axis). Points lying along the diagonal line indicate that the age distributions of the two countries are similar at those quantiles. Most points lie near the diagonal line for ages below 60, indicating similar distributions for these ages. For ages above 60, the points deviate from the line, suggesting differences in the upper age distributions.

## 3.
```{r}
# Calculate the proportion of samples who have lower Self scores than Moses
China$self_Moses <- China$self < China$moses
Mexico$self_Moses <- Mexico$self < Mexico$moses

prop_lower_China <- sum(China$self_Moses == TRUE)/nrow(China)
prop_lower_Mexico <- sum(Mexico$self_Moses == TRUE)/nrow(Mexico)

# Print the proportions
print(prop_lower_China)
print(prop_lower_Mexico)
```
From the proportion result, 56% of respondents in China rank themselves as having less say than Moses in influencing government decisions, while only about 25% of respondents in Mexico feel the same way.  

If we compare this result to the self-assessment response alone, Chinese citizens actually feel they will have more to say to get the government to address issues that interest them according to the graph in question 1. However, in this case where we filter the data about subjects who have lower self score than Moses, we can see more Chinese citizens feel they can say more about getting the government to address the issue than Moses' case.  

The discrepancy between the self-assessment and the vignette-based ranking might indicate that Chinese respondents interpret their political efficacy differently when comparing their situation directly to a vignette. The self-assessment may be influenced by general optimism or social desirability bias, whereas the vignette comparison provides a more grounded assessment of their perceived influence.

## 4.
```{r}
# Samples who ranked the three vignettes in the following order
expected_order_china <- China %>%
  filter(alison >= jane & jane >= moses) %>%
  mutate(self_rank = case_when(
    self < moses ~ 1, # 1 less than moses
    self < jane ~ 2, # 2 equal to moses but less than jane
    self < alison ~ 3, # 3 same as jane but less than alison
    TRUE ~ 4
  ))

expected_order_mexico <- Mexico %>%
  filter(alison >= jane & jane >= moses) %>%
  mutate(self_rank = case_when(
    self < moses ~ 1,
    self < jane ~ 2,
    self < alison ~ 3,
    TRUE ~ 4
  ))

# Create bar plots for the new variable
barplot(prop.table(table(expected_order_china$self_rank)),
        names = c("1", "2", "3", "4"),
        xlab = "Self response in the expected rank order",
        ylab = "Proportion of respondents for each response category", 
        main = "Self-Assessment Responses of Chinese",
        col = colours)

barplot(prop.table(table(expected_order_mexico$self_rank)),
        names = c("1", "2", "3", "4"),
        xlab = "Self response in the expected rank order",
        ylab = "Proportion of respondents for each response category", 
        main = "Self-Assessment Responses of Mexicans",
        col = colours)

# Compute the mean value of the new variable self_rank
mean_self_rank_China <- mean(expected_order_china$self_rank)
mean_self_rank_Mexico <- mean(expected_order_mexico$self_rank)

print(mean_self_rank_China)
print(mean_self_rank_Mexico)

```
The mean value of self_rank this time is quite different from what we have observed in question 1. In question 1, we have the mean of self-assessment response equal to 2.621908 and 1.825301 for China and Mexico respectively. Here, we have 2.150602 and 2.927492 for China and Mexico respectively. Mexican citizens report a higher mean of political efficacy score according to the expected order, with the majority citizens report their self scores at level 3 (self score is ranked the same as Jane or between Jane and Alison). On the other hand, the majority of Chinese citizens report their self scores at level 1 (respondents rank themselves less than Moses).  
The differences in means and distributions indicate that Mexican respondents generally feel they have more political efficacy compared to Chinese respondents. This pattern aligns with the historical context provided: Mexican citizens were able to vote out a long-standing party in 2000, indicating a sense of political agency, while Chinese citizens have not had the same opportunities for political expression.

## 5.
```{r}

# Filter data for age groups
older_respondents_China <- expected_order_china %>% filter(age >= 40)
older_respondents_Mexico <- expected_order_mexico %>% filter(age >= 40)
younger_respondents_China <- expected_order_china %>% filter(age < 40)
younger_respondents_Mexico <- expected_order_mexico %>% filter(age < 40)
xlab = "Self-Reported Political Efficacy Compared to Vignettes"
ylab = "Proportion of Respondents"
names <- c("1", "2", "3", "4")

barplot(prop.table(table(older_respondents_China$self_rank)), ylim = c(0, 0.6), xlab = xlab, ylab = ylab, main = "China (above 40)", names = names, col = colours)

barplot(prop.table(table(older_respondents_Mexico$self_rank)), ylim = c(0, 0.6), xlab = xlab, ylab = ylab, main = "Mexico (above 40)", names = names, col = colours)

barplot(prop.table(table(younger_respondents_China$self_rank)), ylim = c(0, 0.6), xlab = xlab, ylab = ylab, main = "China (below 40)", names = names, col = colours)

barplot(prop.table(table(younger_respondents_Mexico$self_rank)), ylim = c(0, 0.6), xlab = xlab, ylab = ylab, main = "Mexico (below 40)", names = names, col = colours)

# Calculate the mean ranking for each group
mean_ranking_older_china <- mean(older_respondents_China$self_rank)
mean_ranking_older_mexico <- mean(older_respondents_Mexico$self_rank)
mean_ranking_younger_china <- mean(younger_respondents_China$self_rank)
mean_ranking_younger_mexico <- mean(younger_respondents_Mexico$self_rank)


# Calculate the mean self score for each group
mean_self_older_china <- mean(older_respondents_China$self)
mean_self_older_mexico <- mean(older_respondents_Mexico$self)
mean_self_younger_china <- mean(younger_respondents_China$self)
mean_self_younger_mexico <- mean(younger_respondents_Mexico$self)

results <- data.frame(
  Group = c("Older_China", "Older_Mexico", "Younger_China", "Younger_Mexico"),
  Mean_Ranking = c(mean_ranking_older_china, mean_ranking_older_mexico, mean_ranking_younger_china, mean_ranking_younger_mexico),
  Mean_Self_Score = c(mean_self_older_china, mean_self_older_mexico, mean_self_younger_china, mean_self_younger_mexico)
)

results
```
The older Chinese respondents have a slightly higher mean self-score (2.714286) compared to the younger respondents (2.409836), indicating they feel they have more say in government decisions. However, the mean ranking is higher for the older group (2.209524) compared to the younger group (2.049180), suggesting the older group tends to rank themselves less favorably compared to the vignettes.  

The older Mexican respondents have a slightly lower mean self-score (1.744681) compared to the younger respondents (1.773684), indicating both groups feel similarly about their influence on government decisions. The mean ranking is similar between the older (2.921986) and younger (2.931579) groups, showing little difference in how they rank themselves compared to the vignettes.  

For both age groups, respondents in China have higher mean self-scores than those in Mexico, suggesting a higher perceived political efficacy in China. However, the mean rankings indicate that Chinese respondents tend to rank themselves lower compared to the vignettes than Mexican respondents, indicating a potential discrepancy in self-assessment versus vignette-assessment.  

The problem identified in previous questions about different interpretations of political efficacy seems to be more or less severe depending on the age group. In China, older respondents feel they have more influence, whereas in Mexico, the difference between age groups is minimal. This suggests that age might play a role in perceived political efficacy, and this difference is more pronounced in China than in Mexico.  

# Problem 2 - Election and Conditional Cash Transfer in Mexico

```{r Simple ATE function}
diff_in_means <- function(treated, control){
  # Point Estimate
  point <- mean(treated) - mean(control)
  
  # Standard Error
  se <- sqrt(var(treated)/length(treated) + var(control)/length(control))
  
  # Asymptotic 95% CI
  ci_95 <- c(point - qnorm(.975)*se,
             point + qnorm(.975)*se)
  
  # P-value 
  pval <- 2*pnorm(-abs(point/se))

  # Return as a data frame
  output <- data.frame(meanTreated = mean(treated), meanControl = mean(control), est = point, se = se, ci95Lower = ci_95[1], ci95Upper = ci_95[2], pvalue = pval, N= length(treated) + length(control))
  
  return(as_tibble(output))
}
```

## 1.
```{r}
progresa <- read_csv("progresa.csv", show_col_types = FALSE)

treatment <- progresa %>%
  filter(treatment == 1)

control <- progresa %>%
  filter(treatment == 0)

diff_in_means(treatment$t2000, control$t2000)

turnout_model <- lm(t2000 ~ treatment, data = progresa)
summary(turnout_model)

support_model <- lm(pri2000s ~ treatment, data = progresa)
summary(support_model)
```
Simple ATE estimate: From the difference in means between treatment and control group, we can see that there is a positive treatment effect for sure. However, the 95% confidence interval includes 0 so we fail to reject the null hypothesis that there is no impact of the CCT program on turnout and support for the incumbent party (PRI).

Linear regression: From the linear regression result where we predict the treatment effect on both turnout in the 2000 election as a share of precinct and PRI votes as a share of precinct, we can see a positive treatment effect for both (as the coefficient is positive). However, we fail to reject the null hypothesis in both models because the p-value is greater than 0.05, so we cannot conclude that the CCT program will increase in turnout and support for the incumbent party.

## 2.
```{r}
# 
turnout_model <- lm(t2000 ~ treatment + avgpoverty + pobtot1994 + votos1994 + pri1994 + pan1994 + prd1994, data = progresa)
summary(turnout_model)

support_model <- lm(pri2000s ~ treatment + avgpoverty + pobtot1994 + votos1994 + pri1994 + pan1994 + prd1994, data = progresa)
summary(support_model)

```
Turnout (t2000): The Progresa treatment has a positive treatment effect (4.5494445) but not statistically significant effect on turnout (p = 0.147454).
Support for PRI (pri2000s): The Progresa treatment has a positive treatment effect (2.9277395) and marginally significant effect on PRI support (p = 0.08697). 
These results from question 2 are different from question 1 numerically, which has a treatment effect of 3.622 and p-value of 0.0572. Both cases suggest a positive treatment effect on support for PRI, but neither provides strong statistical evidence of significance at the conventional 0.05 level. The additional covariates in the models for Question 2 provide a more nuanced understanding of the treatment effects but do not substantially change the overall interpretation regarding statistical significance.

## 3.
```{r}
turnout_model <- lm(t2000 ~ treatment + avgpoverty + log(pobtot1994) + t1994 + pri1994s + pan1994s + prd1994s, data = progresa)
summary(turnout_model)

support_model <- lm(pri2000s ~ treatment + avgpoverty + log(pobtot1994) + t1994 + pri1994s + pan1994s + prd1994s, data = progresa)
summary(support_model)
```
Turnout (t2000): The treatment effect changes from positive (4.5494445) in the previous model to a small negative (-0.1530) in the new model. The new model shows no significant effect of treatment on turnout, and it improves the model fit significantly with a higher R-squared value (0.6921 compared to 0.0785).  
Support for PRI (pri2000s): The treatment effect remains positive but is reduced from 2.9277395 to 0.23547 in the new model. The new model shows no significant effect of treatment on PRI support, but the fit of the model improves significantly with a higher R-squared value (0.5794 compared to 0.2206).  
The new model specifications, which include the log-transformed precinct population and previous election outcomes as shares of the voting age population, provide a better fit to the data as indicated by higher R-squared values. The results indicate that while the Progresa treatment does not have a significant effect on turnout or PRI support, the inclusion of additional predictors provides a more nuanced understanding of the factors influencing these outcomes. The new model fits the data better compared to the previous models.  

## 4.
```{r balance check}
# Standardize covariates
# progresa <- progresa %>%
#   mutate(treatment_std = treatment/sd(treatment),
#          avgpoverty_std = avgpoverty/sd(avgpoverty),
#          pobtot1994_std = pobtot1994/sd(pobtot1994),
#          votos1994_std = votos1994/sd(votos1994),
#          t1994_std = t1994/sd(t1994),
#          pri1994s_std = pri1994s/sd(pri1994s),
#          pan1994s_std = pan1994s/sd(pan1994s),
#          prd1994s_std = prd1994s/sd(prd1994s))

# Box plot for precinct population
boxplot(pobtot1994 ~ treatment, data = progresa,
        main = "Box Plot of Precinct Population by Treatment Group",
        xlab = "Treatment Group", ylab = "Precinct Population (1994)",
        names = c("Control", "Treatment"))

# Box plot for average poverty index
boxplot(avgpoverty ~ treatment, data = progresa,
        main = "Box Plot of Average Poverty Index by Treatment Group",
        xlab = "Treatment Group", ylab = "Average Poverty Index",
        names = c("Control", "Treatment"))

# Box plot for previous turnout rate
boxplot(t1994 ~ treatment, data = progresa,
        main = "Box Plot of Previous Turnout Rate by Treatment Group",
        xlab = "Treatment Group", ylab = "Previous Turnout Rate (1994)",
        names = c("Control", "Treatment"))

# Box plot for previous PRI support rate
boxplot(pri1994s ~ treatment, data = progresa,
        main = "Box Plot of Previous PRI Support Rate by Treatment Group",
        xlab = "Treatment Group", ylab = "Previous PRI Support Rate (1994)",
        names = c("Control", "Treatment"))

```
The box plots indicate that the key pretreatment variables (precinct population, average poverty index, previous turnout rate, and previous PRI support rate) are generally balanced between the treatment and control groups. This suggests that the randomization process in the Progresa study was effective in creating comparable groups with respect to these covariates. The presence of some outliers, especially in precinct population, does not appear to significantly affect the overall balance between the groups.

## 5.
```{r}
turnout_model_official <- lm(t2000r ~ treatment + avgpoverty + log(pobtot1994) + t1994r + pri1994v + pan1994v + prd1994v, data = progresa)
summary(turnout_model_official)

pri_model_official <- lm(pri2000v ~ treatment + avgpoverty + log(pobtot1994) + t1994r + pri1994v + pan1994v + prd1994v, data = progresa)
summary(pri_model_official)

```

## 6.
```{r}
progresa <- progresa %>%
  mutate(
    poverty_squared = avgpoverty^2,
    treatment_poverty = treatment * avgpoverty,
    treatment_poverty_squared = treatment * (avgpoverty)^2
  )

poverty_interaction_model <- lm(t2000 ~ treatment_poverty + treatment_poverty_squared + log(pobtot1994), data = progresa)
summary(poverty_interaction_model)

```
```{r}
# Define a range of poverty levels for prediction
poverty_levels <- seq(min(progresa$avgpoverty), max(progresa$avgpoverty), length.out = 100)

# Create a data frame for prediction
prediction_data <- data.frame(
  treatment = 1,
  avgpoverty = poverty_levels,
  poverty_squared = poverty_levels^2,
  treatment_poverty = 1 * poverty_levels,
  treatment_poverty_squared = 1 * poverty_levels^2,
  pobtot1994 = mean(progresa$pobtot1994)
)

# Predict the treatment effect at different poverty levels
predicted_effects <- predict(poverty_interaction_model, newdata = prediction_data, se.fit = TRUE)

# Combine predictions with poverty levels
results <- data.frame(
  avgpoverty = poverty_levels,
  treatment_effect = predicted_effects$fit,
  lower_ci = predicted_effects$fit - 1.96 * predicted_effects$se.fit,
  upper_ci = predicted_effects$fit + 1.96 * predicted_effects$se.fit
)

# Plot the predicted treatment effects
ggplot(results, aes(x = avgpoverty, y = treatment_effect)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2) +
  labs(title = "Estimated Treatment Effect by Poverty Level",
       x = "Average Poverty Index",
       y = "Estimated Treatment Effect")

```

# Problem 3 - Data Generating Process
Given the Data Generating Process (DGP):
\[ Y_i \mid X_i, D_i = X_i\beta + \tau D_i + \epsilon_i \]
### Whenever the condition is met,the binary treatment indicator will be: 
\[ D_i \mid X_i = I(X_i\gamma + \nu_i > 0) \]
### The noise term follows a standard normal distribution with mean of 0 and a variance of 1:
\[ \epsilon_i \sim N(0, 1) \]
\[ \nu_i \sim N(0, 1) \]
\[ X_i \sim Bernoulli(\pi) \]

### 3.1 Distribution of \( Y_i \mid X_i, D_i \)

Initially, since \(\epsilon_i \sim N(0, 1)\), the outcome will be normally distributed when the linear combination adds with a random variable that follows standard normal distribution \(N(0, 1)\):

\[ Y_i \mid X_i, D_i \sim N(X_i\beta + \tau D_i, 1) \]

Where the mean is \( \mu_{Y_i \mid X_i, D_i} = X_i\beta + \tau D_i \).

Thus, the mean and variance of \( Y_i \mid X_i, D_i \) are:  
- **Mean**: \( E[Y_i \mid X_i, D_i] = X_i\beta + \tau D_i \).  
- **Variance**: \( \text{Var}(Y_i \mid X_i, D_i) = 1 \).  

Secondly, since the outcome variable is \(E[Y_i \mid X_i, D_i] \sim N(X_i\beta + \tau D_i, 1) \), we can standardize it by the following:  
- Let \( A = E[Y_i \mid X_i, D_i]\) and since \( Z = \frac{A - \mu}{\sigma}\).  
- We will have \( Z = \frac{A - (X_i\beta + \tau D_i) }{1} \) since the mean and variance of \( Y_i \mid X_i, D_i \) are \(X_i\beta + \tau D_i\) and 1.  
- And therefore \( Z \sim N(0, 1)\).  

Lastly, given \( Z \sim N(0, 1)\) and \( Z = A - (X_i\beta + \tau D_i)\), the probability density function (PDF) and cumulative distribution function (CDF) are:  
**PDF** Here we use the PDF of a normal distribution \( N(\mu, \sigma^2)\):  
- \( f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)\).  
We will have the following evaluated at point y with \( N(0, 1)\):  
- \( \phi(y) = \frac{1}{\sqrt{2\pi}} \exp\left( -\frac{(y - (X_i\beta + \tau D_i))^2}{2} \right)\).  

**CDF** Here we use the CDF of a normal distribution \( N(\mu, \sigma^2 )\):  
- \( F(x) =  \int_{-\infty}^{x} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(t - \mu)^2}{2\sigma^2}\right) dt \).  
We will have the following CDF evaluated at point z with \( N(0, 1)\):  
- \( \Phi(x) = \Pr(Z \leq x) = \int_{-\infty}^{x} \phi(t) dt \).  
- Therefore, we will have: \(F(y) = \Phi(y - (X_i\beta + \tau D_i))\).  

### 3.2 Expected Values of \( D_i \mid X_i \)

Initially, \(D_i\) is an indicator function that takes the value 1 when \(X_i \gamma + \nu_i > 0\), and 0 otherwise. Hence, \(\nu_i > - X_i \gamma\). Secondly, \(X_i\), our covariate, is a binary random variable that takes value 1 with probability \(\pi\).  

For the first case when \(E[D_i \mid X_i = 1]\):  
- \( E[D_i \mid X_i = 1] = \Pr(D_i = 1 \mid X_i = 1) \) because the expected value of an indicator function is the probability that the condition is met, which will be \(\Pr(\nu_i > -\gamma)\) in this case.  
- Since \( \nu_i \sim N(0, 1)\):  
\(\Pr(\nu_i > -\gamma) = 1 - \Pr(\nu_i \leq -\gamma) = \Phi(\gamma)\).  
- In conclusion, \( E[D_i \mid X_i = 1] = \Phi(\gamma) \).  

For the second case when \(E[D_i \mid X_i = 0]\):  
- Given \(X_i = 0\), the condition that \(D_i = 1\) is when \(\nu_i > 0\).  
- \( E[D_i \mid X_i = 0] = \Pr(D_i = 1 \mid X_i = 0) = \Pr(\nu_i > 0) = 1 - \Pr(\nu_i \leq 0)\).  
- Thus, \(\Pr(\nu_i \leq 0) = \Phi(0) = 0.5\) and \( E[D_i \mid X_i = 0] = 1 - \Phi(0) = 1 - 0.5 = 0.5\).  

In summary, we find that \( E[D_i \mid X_i = 1] = \Phi(\gamma) \) and \( E[D_i \mid X_i = 0] = 0.5\).  

### 3.3 Applying Bayes' Rule

Bayes' rule states that:  
\[ \Pr(A = a \mid B = b) = \frac{\Pr(B = b \mid A = a) \Pr(A = a)}{\sum_{a} \Pr(B = b \mid A = a)\Pr(A = a)} \] where \( \Pr(B = b) = \sum_{a} \Pr(B = b \mid A = a)\Pr(A = a) \) according to the law of total probability.  

From previous questions, we have found out that:  
- \( E[D_i \mid X_i = 1] = \Phi(\gamma) \) and \( E[D_i \mid X_i = 0] = 0.5\).  

For the first case, we want \( \Pr(X_i = 1 \mid D_i = 1)\):  
\[ \Pr(X_i = 1 \mid D_i = 1) = \frac{\Pr(D_i = 1 \mid X_i = 1) \Pr(X_i = 1)}{\sum_{x}\Pr(D_i = 1 \mid X_i = x)\Pr(X_i = x)} \]  
Given \( \Pr(D_i = 1 \mid X_i = 1) = \Phi(\gamma) \), \( \Pr(D_i = 1 \mid X_i = 0) = 0.5 \), \( \Pr(X_i = 1) = \pi \), and \( \Pr(X_i = 0) = 1 - \pi\), we will have:  
\[ \frac{\Pr(D_i = 1 \mid X_i = 1) \Pr(X_i = 1)}{\sum_{x}\Pr(D_i = 1 \mid X_i = x)\Pr(X_i = x)} = \frac{\Phi(\gamma)\pi}{\Phi(\gamma)\pi + 0.5(1 - \pi)} \]  
In conclusion, \( \Pr(X_i = 1 \mid D_i = 1) = \frac{\Phi(\gamma)\pi}{\Phi(\gamma)\pi + 0.5(1 - \pi)} \)  

Similarly, for \( \Pr(X_i = 1 \mid D_i = 0) \):  
\[ \Pr(X_i = 1 \mid D_i = 0) = \frac{(1 -\Phi(\gamma))\pi}{1 - (\Phi(\gamma)\pi + 0.5(1 - \pi))} \] 

### 3.4 Deriving Bias Formula

For \( E[Y_i \mid D_i = 1] \) and \( E[Y_i \mid D_i = 0] \), we initially substitute equation (1) from the DGP:

- \( E[Y_i \mid D_i = 1] = E[X_i\beta + \tau D_i + \epsilon_i] = X_i\beta + \tau \) as \( E[\epsilon_i] = 0 \)
- \( E[Y_i \mid D_i = 0] = E[X_i\beta + \tau D_i + \epsilon_i] = X_i\beta \)

Thus, we have:  
- \( \eta = (X_i\beta + \tau) - X_i\beta = \tau \)  

Therefore, the formula for the bias \( \eta - \tau \) will be \( \eta - \tau = \tau - \tau = 0 \), which shows that there is no bias in the DGP equations.


# Problem 4 - Propensity Score Regression and IPW

### 4.1
The propensity score is the probability of receiving treatment given the covariate, \(e(X_i) = P(D_i = 1 \mid X_i)\). For \(X_i = 1\) and \(X_i = 0\), the predicted propensity scores can be calculated as follows, given \( (X'X)^{-1}X'D = \frac{1}{\sum_{i=1}^{n}X_i} \) and \( X'D = \sum_{i=1}^{n}X_i D_i \) :  
- For \(X = 1\):  
The propensity score is the probability of receiving treatment given the covariate is \( X = 1 \). And we know from DGP before that X is a binary random variable. Therefore, the propensity score is the sum of treatment given the covariate is 1 divided by the number of units who has the covariate of 1.  
\[ \hat{e}(1) = E[D_i \mid X_i = 1] =\gamma_i =\frac{\sum_{i=1}^{n} X_i D_i}{\sum_{i=1}^{n} X_i} \]

- For \(X = 0\):  
Similarly, the propensity score here is \( \hat{e}(0) = Pr(D_i = 1 \mid X_i = 0) \), which can be calculated by taking the sum of \( D_i\) for all instances where \(X_i = 0\) and dividing it by the total number of instances where \(X_i = 0\).  
\[ \hat{e}(0) = E[D_i \mid X_i = 0] =\gamma_0 = \frac{\sum_{i=1}^{n} (1 - X_i) D_i}{\sum_{i=1}^{n} (1 - X_i)} \]

### 4.2
Based on the information provided, we know that:  
\[ \hat{e}(1) = \frac{\sum_{i=1}^{n} X_i D_i}{\sum_{i=1}^{n} X_i} \]   
\[ \hat{e}(0) = \frac{\sum_{i=1}^{n} (1 - X_i) D_i}{\sum_{i=1}^{n} (1 - X_i)} \]   

- For the stratified estimator on X:  
The stratified estimator is the sum of average treatment effect of all stratas and their corresponding weight (the number of units in the strata divided by the total number of all units).  

- For the IPW estimator:  
\(\hat{\tau} = \frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1 - D_i) Y_i}{1 - e(X_i)} \right) \)

Given \[ \hat{e}(1) = \frac{\sum_{i=1}^{n} X_i D_i}{\sum_{i=1}^{n} X_i} \], \[ X_i \sim Bernoulli(\pi) \], and the IPW estimator, we will have:  
- For \( X_i = 1 \):  
\(\hat{\tau} = \frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i}{\hat{e}(1)} - \frac{(1 - D_i) Y_i}{1 - \hat{e}(1)} \right) \)   

- For \( X_i = 0 \):  
\(\hat{\tau} = \frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i}{\hat{e}(0)} - \frac{(1 - D_i) Y_i}{1 - \hat{e}(0)} \right) \)  

Since we know that the propensity score is the sum of \( D_i\) for all instances where \(X_i = 1\) and dividing it by the total number of instances where \(X_i = 1\), we will have:  
\[ \hat{e}(1) = \frac{\sum_{i: X_i = 1} D_i}{N_{X = 1}} \]  
Similarly, we will also have:  
\[ \hat{e}(0) = \frac{\sum_{i: X_i = 0} D_i}{N_{X = 0}} \]  

Substitute both equation into \(\hat{\tau}\):  
- For \( X_i = 1 \):  
\(\frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i}{\frac{\sum_{i: X_i = 1} D_i}{N_{X = 1}}} - \frac{(1 - D_i) Y_i}{1 - \frac{\sum_{i: X_i = 1} D_i}{N_{X = 1}}} \right) \)  
which is equivalent to:  
\( \frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i N_{X = 1}}{\sum_{i: X_i = 1} D_i} - \frac{(1 - D_i) Y_i N_{X = 1}}{N_{X = 1} - {\sum_{i: X_i = 1} D_i}} \right) \)  
Therefore, we will have the following after simplification:  
\( \frac{N_{X=1}}{N} \left( \frac{ \sum_{i: X_i = 1} D_i Y_i }{\sum_{i: X_i = 1} D_i} - \frac{\sum_{i: X_i = 1}(1 - D_i) Y_i}{N_{X = 1} - {\sum_{i: X_i = 1} D_i}} \right) \)  
Eventually:  
\( \frac{1}{N_{X_i = 1 D_i = 1}} \sum_{i: X_i = 1} D_i Y_i -\frac{1}{N_{X_i = 1 D_i = 1}} \sum_{i: X_i = 0}(1 - D_i) Y_i\)  

- For \( X_i = 0 \): 
\(\frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i}{\frac{\sum_{i: X_i = 0} D_i}{N_{X = 0}}} - \frac{(1 - D_i) Y_i}{1 - \frac{\sum_{i: X_i = 0} D_i}{N_{X = 0}}} \right) \)  
which is equivalent to:  
\( \frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i N_{X = 0}}{\sum_{i: X_i = 0} D_i} - \frac{(1 - D_i) Y_i N_{X = 0}}{N_{X = 0} - {\sum_{i: X_i = 0} D_i}} \right) \)  
Therefore, we will have the following after simplification:  
\( \frac{N_{X=0}}{N} \left( \frac{ \sum_{i: X_i = 0} D_i Y_i }{\sum_{i: X_i = 0} D_i} - \frac{\sum_{i: X_i = 0}(1 - D_i) Y_i}{N_{X = 0} - {\sum_{i: X_i = 0} D_i}} \right) \)  
Eventually:  
\( \frac{1}{N_{X_i = 0 D_i = 1}} \sum_{i: X_i = 0} D_i Y_i -\frac{1}{N_{X_i = 0 D_i = 1}} \sum_{i: X_i = 0}(1 - D_i) Y_i\)  

Thus, if we combine everything together:  
\(\hat{\tau} = \frac{1}{n}\sum_{i = 1}^{n} \left( \frac{D_i Y_i}{e(X_i)} - \frac{(1 - D_i) Y_i}{1 - e(X_i)} \right) =  \frac{1}{N_{X_i = 1 D_i = 1}} \sum_{i: X_i = 1} D_i Y_i -\frac{1}{N_{X_i = 1 D_i = 1}} \sum_{i: X_i = 0}(1 - D_i) Y_i +
\frac{1}{N_{X_i = 0 D_i = 1}} \sum_{i: X_i = 0} D_i Y_i - \frac{1}{N_{X_i = 0 D_i = 1}} \sum_{i: X_i = 0}(1 - D_i) Y_i\)
Simplify:  
\[ = \sum_{x=0}^{1} \frac{N_{X = x}}{n} \left( \frac{1}{N_{X_i = x, D_i = 1}} \sum_{i : X_i = x} Y_i D_i - \frac{1}{N_{X_i = x, D_i = 0}} \sum_{i : X_i = x} Y_i (1 - D_i) \right)\]  

### 4.3


# Problem 5 - Matching Approach using TRC

### Part a. 
```{r}
# Loading library needed for reading dta file and matching
# install.packages("Matching")
library(haven)
library(Matching)
# Reading the data
trc <- read_dta("trc_data.dta")

# Treatment of our interest
treatment <- trc$TRCKNOW

# Outcome of our interest
Y <- trc$RUSTAND

# The covariates we need for matching
covariates <- trc[, c("age", "female", "wealth", "religiosity", "ethsalience", "rcblack", "rcwhite", "rccol", "EDUC")]

# Apply matching function in R, Weight = 2 means specifies Mahalanobis distance
match_result <- Match(Y = Y, Tr = treatment, X = covariates, estimand ="ATE", M = 1, Weight = 2)
summary(match_result)

# The estimate of causal effect
ate <- match_result$est
confidence_interval <- match_result$se * qnorm(0.975)

cat("Estimated ATE:", ate, "\n")
cat("95% Confidence Interval:", ate - confidence_interval, "to", ate + confidence_interval, "\n")

```

### Part b.
```{r}
match_result <- Match(Y = Y, Tr = treatment, X = covariates, estimand ="ATE", M = 3, Weight = 2)
summary(match_result)

# The estimate of causal effect
ate <- match_result$est
confidence_interval <- match_result$se * qnorm(0.975)

cat("Estimated ATE:", ate, "\n")
cat("95% Confidence Interval:", ate - confidence_interval, "to", ate + confidence_interval, "\n")
```
The standard error changes and decreases from 0.050924 to 0.046918.

### Part c.
```{r}
match_result <- Match(Y = Y, Tr = treatment, X = covariates, estimand ="ATE", BiasAdjust = TRUE, M = 3, Weight = 2)
summary(match_result)

# The estimate of causal effect
ate <- match_result$est
confidence_interval <- match_result$se * qnorm(0.975)

cat("Estimated ATE:", ate, "\n")
cat("95% Confidence Interval:", ate - confidence_interval, "to", ate + confidence_interval, "\n")
```
The standard error decreases after implementing bias adjust.

### Part d.
```{r}

# # Checking balance between treatment and control group, so that our treatment and control will have the same distribution
# MatchBalance(TRCKNOW ~ age + female + wealth + religiosity + ethsalience + rcblack + rcwhite + rccol + EDUC, data= trc, match.out = match_result)

# Extract indices of matched pairs
treated_indices <- match_result$index.treated
control_indices <- match_result$index.control

# Compute matching weights
weights <- rep(0, nrow(trc))
weights[treated_indices] <- 1 / length(treated_indices) # Weights for treated units
weights[control_indices] <- 1 / length(control_indices) # Weights for control units

# Add weights to the dataset
trc$weights <- weights

# Function to compute weighted mean and standard deviation
weighted_stats <- function(var, weights) {
  weighted_mean <- sum(var * weights) / sum(weights)
  weighted_sd <- sqrt(sum(weights * (var - weighted_mean)^2) / sum(weights))
  return(c(mean = weighted_mean, sd = weighted_sd))
}

# Compute balance statistics for each covariate
balance_table <- data.frame(Covariate = colnames(covariates),
                            Treated_Mean = NA, Treated_SD = NA,
                            Control_Mean = NA, Control_SD = NA)

for (i in 1:ncol(covariates)) {
  covariate <- covariates[, i]
  treated_stats <- weighted_stats(trc[treatment == 1, i], trc$weights[treatment == 1])
  control_stats <- weighted_stats(trc[treatment == 0, i], trc$weights[treatment == 0])
  
  balance_table$Treated_Mean[i] <- treated_stats["mean"]
  balance_table$Treated_SD[i] <- treated_stats["sd"]
  balance_table$Control_Mean[i] <- control_stats["mean"]
  balance_table$Control_SD[i] <- control_stats["sd"]
}


# Display the balance table
print(balance_table)

```

